{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA) for Customer Churn Prediction\n",
    "\n",
    "This notebook provides a comprehensive exploratory data analysis of customer churn datasets, including:\n",
    "- Univariate analysis of all features\n",
    "- Bivariate analysis with statistical testing\n",
    "- Correlation analysis using appropriate measures\n",
    "- Publication-ready visualizations\n",
    "- Business insights and recommendations\n",
    "\n",
    "## Table of Contents\n",
    "1. [Setup and Data Loading](#setup)\n",
    "2. [Data Overview](#overview)\n",
    "3. [Univariate Analysis](#univariate)\n",
    "4. [Bivariate Analysis](#bivariate)\n",
    "5. [Correlation Analysis](#correlation)\n",
    "6. [Statistical Testing](#testing)\n",
    "7. [Key Insights and Business Recommendations](#insights)\n",
    "8. [Conclusions](#conclusions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'config'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m stats\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Import our custom modules\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdata_prep\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataLoader, DataCleaner, DataSplitter\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01meda\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m EDAAnalyzer, StatisticalTester, EDAVisualizer\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Github/Enhanced-Customer-Churn-Analysis-with-ML-CLV-and-Explainability/notebooks/../src/data_prep.py:17\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split, StratifiedShuffleSplit\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwarnings\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Set up logging\u001b[39;00m\n\u001b[32m     20\u001b[39m logging.basicConfig(level=\u001b[38;5;28mgetattr\u001b[39m(logging, config.LOG_LEVEL), \u001b[38;5;28mformat\u001b[39m=config.LOG_FORMAT)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'config'"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src and root to path for imports\n",
    "notebook_dir = Path.cwd()\n",
    "if notebook_dir.name == 'notebooks':\n",
    "    project_root = notebook_dir.parent\n",
    "else:\n",
    "    project_root = notebook_dir\n",
    "\n",
    "sys.path.insert(0, str(project_root))\n",
    "sys.path.insert(0, str(project_root / 'src'))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "# Import our custom modules\n",
    "try:\n",
    "    from data_prep import DataLoader, DataCleaner, DataSplitter\n",
    "    from eda import EDAAnalyzer, StatisticalTester, EDAVisualizer\n",
    "    from config import config\n",
    "    print(\"‚úÖ Custom modules imported successfully!\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Import error: {e}\")\n",
    "    print(\"Please ensure you're running this notebook from the project root or notebooks directory\")\n",
    "    raise\n",
    "\n",
    "# Configure display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "\n",
    "# Configure plotting\n",
    "try:\n",
    "    plt.style.use('seaborn-v0_8')\n",
    "except:\n",
    "    plt.style.use('default')\n",
    "    print(\"Using default matplotlib style (seaborn-v0_8 not available)\")\n",
    "\n",
    "sns.set_palette(\"husl\")\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(config.RANDOM_SEED)\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"Working directory: {Path.cwd()}\")\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Config loaded - Random seed: {config.RANDOM_SEED}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading {#setup}\n",
    "\n",
    "First, let's import the necessary libraries and load our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Prepare Data\n",
    "\n",
    "We'll load both Telco and Olist datasets to demonstrate the EDA framework on different data structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize data components\n",
    "loader = DataLoader()\n",
    "cleaner = DataCleaner()\n",
    "\n",
    "print(\"Loading Telco Customer Churn dataset...\")\n",
    "try:\n",
    "    # Load Telco data\n",
    "    telco_raw = loader.load_telco_data()\n",
    "    print(f\"Telco dataset loaded: {telco_raw.shape}\")\n",
    "    \n",
    "    # Clean the data\n",
    "    telco_clean = cleaner.clean_telco_data(telco_raw)\n",
    "    print(f\"Telco dataset cleaned: {telco_clean.shape}\")\n",
    "    \n",
    "    telco_available = True\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Could not load Telco dataset: {e}\")\n",
    "    print(\"Will create synthetic data for demonstration...\")\n",
    "    \n",
    "    # Create synthetic Telco-like data for demonstration\n",
    "    np.random.seed(42)\n",
    "    n_samples = 1000\n",
    "    \n",
    "    telco_clean = pd.DataFrame({\n",
    "        'customerID': [f'C{i:04d}' for i in range(n_samples)],\n",
    "        'gender': np.random.choice(['Male', 'Female'], n_samples),\n",
    "        'SeniorCitizen': np.random.choice([0, 1], n_samples, p=[0.8, 0.2]),\n",
    "        'Partner': np.random.choice([0, 1], n_samples, p=[0.5, 0.5]),\n",
    "        'Dependents': np.random.choice([0, 1], n_samples, p=[0.7, 0.3]),\n",
    "        'tenure': np.random.exponential(20, n_samples).astype(int),\n",
    "        'PhoneService': np.random.choice([0, 1], n_samples, p=[0.1, 0.9]),\n",
    "        'InternetService': np.random.choice(['No', 'DSL', 'Fiber optic'], n_samples, p=[0.2, 0.4, 0.4]),\n",
    "        'Contract': np.random.choice(['Month-to-month', 'One year', 'Two year'], n_samples, p=[0.5, 0.3, 0.2]),\n",
    "        'PaperlessBilling': np.random.choice([0, 1], n_samples, p=[0.4, 0.6]),\n",
    "        'PaymentMethod': np.random.choice(['Electronic check', 'Mailed check', 'Bank transfer', 'Credit card'], n_samples),\n",
    "        'MonthlyCharges': np.random.normal(65, 20, n_samples).clip(20, 120),\n",
    "        'TotalCharges': None  # Will calculate based on tenure and monthly charges\n",
    "    })\n",
    "    \n",
    "    # Calculate TotalCharges with some noise\n",
    "    telco_clean['TotalCharges'] = (telco_clean['tenure'] * telco_clean['MonthlyCharges'] * \n",
    "                                  np.random.normal(1, 0.1, n_samples)).clip(0, None)\n",
    "    \n",
    "    # Create churn target with realistic patterns\n",
    "    churn_prob = (\n",
    "        0.1 +  # Base rate\n",
    "        0.3 * (telco_clean['Contract'] == 'Month-to-month') +  # Contract effect\n",
    "        0.2 * (telco_clean['tenure'] < 12) +  # Tenure effect\n",
    "        0.1 * (telco_clean['MonthlyCharges'] > 80) +  # Price effect\n",
    "        0.1 * telco_clean['SeniorCitizen']  # Age effect\n",
    "    ).clip(0, 1)\n",
    "    \n",
    "    telco_clean['Churn'] = np.random.binomial(1, churn_prob, n_samples)\n",
    "    \n",
    "    telco_available = True\n",
    "    print(f\"Synthetic Telco dataset created: {telco_clean.shape}\")\n",
    "\n",
    "# Display basic info about the dataset\n",
    "if telco_available:\n",
    "    print(\"\\n=== Telco Dataset Info ===\")\n",
    "    print(f\"Shape: {telco_clean.shape}\")\n",
    "    print(f\"Churn rate: {telco_clean['Churn'].mean():.2%}\")\n",
    "    print(f\"Missing values: {telco_clean.isnull().sum().sum()}\")\n",
    "    \n",
    "    # Display first few rows\n",
    "    display(telco_clean.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Overview {#overview}\n",
    "\n",
    "Let's get a comprehensive overview of our dataset structure and quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data quality report\n",
    "quality_report = loader.generate_quality_report(telco_clean, \"Telco\")\n",
    "\n",
    "print(\"=== Data Quality Report ===\")\n",
    "print(f\"Total rows: {quality_report.total_rows:,}\")\n",
    "print(f\"Total columns: {quality_report.total_columns}\")\n",
    "print(f\"Memory usage: {quality_report.memory_usage}\")\n",
    "print(f\"Duplicate rows: {quality_report.duplicate_rows}\")\n",
    "\n",
    "if quality_report.missing_values:\n",
    "    print(\"\\nMissing values:\")\n",
    "    for col, count in quality_report.missing_values.items():\n",
    "        percentage = (count / quality_report.total_rows) * 100\n",
    "        print(f\"  {col}: {count} ({percentage:.1f}%)\")\n",
    "else:\n",
    "    print(\"\\nNo missing values found!\")\n",
    "\n",
    "if quality_report.outliers:\n",
    "    print(\"\\nOutliers detected:\")\n",
    "    for col, count in quality_report.outliers.items():\n",
    "        percentage = (count / quality_report.total_rows) * 100\n",
    "        print(f\"  {col}: {count} ({percentage:.1f}%)\")\n",
    "else:\n",
    "    print(\"\\nNo outliers detected using IQR method.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display data types and basic statistics\n",
    "print(\"=== Data Types ===\")\n",
    "print(telco_clean.dtypes)\n",
    "\n",
    "print(\"\\n=== Basic Statistics ===\")\n",
    "display(telco_clean.describe(include='all'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Univariate Analysis {#univariate}\n",
    "\n",
    "Let's analyze each variable individually to understand their distributions and characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize EDA components\n",
    "eda_analyzer = EDAAnalyzer(significance_level=0.05, correlation_threshold=0.7)\n",
    "visualizer = EDAVisualizer()\n",
    "\n",
    "# Perform univariate analysis\n",
    "print(\"Performing univariate analysis...\")\n",
    "univariate_results = eda_analyzer.perform_univariate_analysis(telco_clean)\n",
    "\n",
    "print(f\"\\nAnalyzed {len(univariate_results)} variables\")\n",
    "\n",
    "# Display results for each variable type\n",
    "numeric_vars = [name for name, result in univariate_results.items() if result.data_type == 'numeric']\n",
    "categorical_vars = [name for name, result in univariate_results.items() if result.data_type == 'categorical']\n",
    "\n",
    "print(f\"\\nNumeric variables ({len(numeric_vars)}): {numeric_vars}\")\n",
    "print(f\"Categorical variables ({len(categorical_vars)}): {categorical_vars}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display detailed results for numeric variables\n",
    "print(\"=== Numeric Variables Analysis ===\")\n",
    "numeric_summary = []\n",
    "\n",
    "for name, result in univariate_results.items():\n",
    "    if result.data_type == 'numeric':\n",
    "        numeric_summary.append({\n",
    "            'Variable': name,\n",
    "            'Missing (%)': f\"{result.missing_percentage:.1f}%\",\n",
    "            'Unique': result.unique_count,\n",
    "            'Mean': f\"{result.mean:.2f}\" if result.mean else 'N/A',\n",
    "            'Median': f\"{result.median:.2f}\" if result.median else 'N/A',\n",
    "            'Std': f\"{result.std:.2f}\" if result.std else 'N/A',\n",
    "            'Skewness': f\"{result.skewness:.2f}\" if result.skewness else 'N/A',\n",
    "            'Kurtosis': f\"{result.kurtosis:.2f}\" if result.kurtosis else 'N/A'\n",
    "        })\n",
    "\n",
    "numeric_df = pd.DataFrame(numeric_summary)\n",
    "display(numeric_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display detailed results for categorical variables\n",
    "print(\"=== Categorical Variables Analysis ===\")\n",
    "categorical_summary = []\n",
    "\n",
    "for name, result in univariate_results.items():\n",
    "    if result.data_type == 'categorical':\n",
    "        categorical_summary.append({\n",
    "            'Variable': name,\n",
    "            'Missing (%)': f\"{result.missing_percentage:.1f}%\",\n",
    "            'Unique': result.unique_count,\n",
    "            'Mode': result.mode,\n",
    "            'Mode Freq': result.mode_frequency,\n",
    "            'Mode (%)': f\"{result.mode_percentage:.1f}%\" if result.mode_percentage else 'N/A'\n",
    "        })\n",
    "\n",
    "categorical_df = pd.DataFrame(categorical_summary)\n",
    "display(categorical_df)\n",
    "\n",
    "# Show top categories for each categorical variable\n",
    "print(\"\\n=== Top Categories by Variable ===\")\n",
    "for name, result in univariate_results.items():\n",
    "    if result.data_type == 'categorical' and result.top_categories:\n",
    "        print(f\"\\n{name}:\")\n",
    "        for category, count in list(result.top_categories.items())[:5]:  # Top 5\n",
    "            percentage = (count / (len(telco_clean) - result.missing_count)) * 100\n",
    "            print(f\"  {category}: {count} ({percentage:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create univariate summary visualization\n",
    "fig_univariate = visualizer.plot_univariate_summary(univariate_results, 'univariate_summary')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Bivariate Analysis {#bivariate}\n",
    "\n",
    "Now let's analyze the relationship between each feature and the target variable (churn)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform bivariate analysis\n",
    "print(\"Performing bivariate analysis...\")\n",
    "target_col = 'Churn'\n",
    "feature_cols = [col for col in telco_clean.columns if col not in ['customerID', target_col]]\n",
    "\n",
    "bivariate_results = eda_analyzer.perform_bivariate_analysis(\n",
    "    telco_clean, target_col, feature_cols\n",
    ")\n",
    "\n",
    "print(f\"\\nAnalyzed {len(bivariate_results)} features against target '{target_col}'\")\n",
    "\n",
    "# Create summary of bivariate results\n",
    "bivariate_summary = []\n",
    "\n",
    "for name, result in bivariate_results.items():\n",
    "    row = {\n",
    "        'Feature': name,\n",
    "        'Type': result.feature_type,\n",
    "        'Test': result.test_name,\n",
    "        'Statistic': f\"{result.test_statistic:.3f}\" if result.test_statistic else 'N/A',\n",
    "        'P-value': f\"{result.p_value:.3e}\" if result.p_value else 'N/A',\n",
    "        'Effect Size': f\"{result.effect_size:.3f}\" if result.effect_size else 'N/A',\n",
    "        'Effect Interpretation': result.effect_size_interpretation or 'N/A',\n",
    "        'Significant': 'Yes' if result.p_value and result.p_value < 0.05 else 'No'\n",
    "    }\n",
    "    \n",
    "    # Add correlation for numeric features\n",
    "    if result.correlation_coefficient is not None:\n",
    "        row['Correlation'] = f\"{result.correlation_coefficient:.3f}\"\n",
    "    \n",
    "    bivariate_summary.append(row)\n",
    "\n",
    "bivariate_df = pd.DataFrame(bivariate_summary)\n",
    "\n",
    "# Sort by significance and effect size\n",
    "bivariate_df['P-value_numeric'] = pd.to_numeric(bivariate_df['P-value'], errors='coerce')\n",
    "bivariate_df = bivariate_df.sort_values(['Significant', 'P-value_numeric'], ascending=[False, True])\n",
    "bivariate_df = bivariate_df.drop('P-value_numeric', axis=1)\n",
    "\n",
    "print(\"\\n=== Bivariate Analysis Results ===\")\n",
    "display(bivariate_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify most significant features\n",
    "significant_features = bivariate_df[bivariate_df['Significant'] == 'Yes']\n",
    "\n",
    "print(f\"\\n=== Most Significant Features ({len(significant_features)} total) ===\")\n",
    "if len(significant_features) > 0:\n",
    "    display(significant_features.head(10))\n",
    "    \n",
    "    print(f\"\\nTop 5 most significant features:\")\n",
    "    for i, (_, row) in enumerate(significant_features.head(5).iterrows()):\n",
    "        print(f\"{i+1}. {row['Feature']} (p={row['P-value']}, effect={row['Effect Interpretation']})\")\n",
    "else:\n",
    "    print(\"No statistically significant features found at Œ±=0.05 level.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bivariate summary visualization\n",
    "fig_bivariate = visualizer.plot_bivariate_summary(bivariate_results, 'bivariate_summary')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detailed Analysis of Top Features\n",
    "\n",
    "Let's create detailed visualizations for the most significant features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot churn rates for top categorical features\n",
    "categorical_significant = significant_features[significant_features['Type'] == 'categorical']\n",
    "\n",
    "if len(categorical_significant) > 0:\n",
    "    print(\"=== Churn Rates by Top Categorical Features ===\")\n",
    "    \n",
    "    for _, row in categorical_significant.head(3).iterrows():\n",
    "        feature = row['Feature']\n",
    "        print(f\"\\n{feature}:\")\n",
    "        \n",
    "        # Calculate and display churn rates\n",
    "        churn_rates = telco_clean.groupby(feature)[target_col].agg(['mean', 'count'])\n",
    "        churn_rates.columns = ['Churn_Rate', 'Count']\n",
    "        churn_rates['Churn_Rate_Pct'] = churn_rates['Churn_Rate'] * 100\n",
    "        \n",
    "        display(churn_rates.sort_values('Churn_Rate', ascending=False))\n",
    "        \n",
    "        # Create visualization\n",
    "        fig = visualizer.plot_churn_rate_by_segments(\n",
    "            telco_clean, feature, target_col, \n",
    "            title=f'Churn Rate by {feature.replace(\"_\", \" \").title()}',\n",
    "            save_name=f'churn_rate_{feature}'\n",
    "        )\n",
    "        plt.show()\n",
    "        plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distributions for top numeric features\n",
    "numeric_significant = significant_features[significant_features['Type'] == 'numeric']\n",
    "\n",
    "if len(numeric_significant) > 0:\n",
    "    print(\"=== Distributions of Top Numeric Features ===\")\n",
    "    \n",
    "    for _, row in numeric_significant.head(3).iterrows():\n",
    "        feature = row['Feature']\n",
    "        print(f\"\\n{feature}:\")\n",
    "        \n",
    "        # Calculate and display summary statistics by churn\n",
    "        summary_stats = telco_clean.groupby(target_col)[feature].describe()\n",
    "        display(summary_stats)\n",
    "        \n",
    "        # Create visualization\n",
    "        fig = visualizer.plot_numeric_distribution_by_churn(\n",
    "            telco_clean, feature, target_col,\n",
    "            title=f'{feature.replace(\"_\", \" \").title()} Distribution by Churn Status',\n",
    "            save_name=f'distribution_{feature}'\n",
    "        )\n",
    "        plt.show()\n",
    "        plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Correlation Analysis {#correlation}\n",
    "\n",
    "Let's analyze correlations between variables using appropriate measures for different data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform correlation analysis\n",
    "print(\"Performing correlation analysis...\")\n",
    "\n",
    "# Use mixed correlation analysis (automatic method selection)\n",
    "correlation_result = eda_analyzer.perform_correlation_analysis(\n",
    "    telco_clean, method='auto', include_categorical=True\n",
    ")\n",
    "\n",
    "print(f\"Correlation method used: {correlation_result.correlation_method}\")\n",
    "print(f\"Correlation matrix shape: {correlation_result.correlation_matrix.shape}\")\n",
    "print(f\"High correlations found: {len(correlation_result.high_correlations)}\")\n",
    "print(f\"Significant correlations found: {len(correlation_result.significant_correlations)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display correlation matrix\n",
    "print(\"=== Correlation Matrix ===\")\n",
    "display(correlation_result.correlation_matrix.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create correlation heatmap\n",
    "fig_correlation = visualizer.plot_correlation_heatmap(\n",
    "    correlation_result.correlation_matrix,\n",
    "    title=f'Correlation Matrix ({correlation_result.correlation_method.title()})',\n",
    "    save_name='correlation_heatmap'\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze high correlations\n",
    "if correlation_result.high_correlations:\n",
    "    print(\"=== High Correlations (|r| > 0.7) ===\")\n",
    "    high_corr_df = pd.DataFrame(correlation_result.high_correlations, \n",
    "                               columns=['Variable 1', 'Variable 2', 'Correlation'])\n",
    "    high_corr_df['Abs_Correlation'] = high_corr_df['Correlation'].abs()\n",
    "    high_corr_df = high_corr_df.sort_values('Abs_Correlation', ascending=False)\n",
    "    \n",
    "    display(high_corr_df.drop('Abs_Correlation', axis=1))\n",
    "    \n",
    "    print(\"\\n‚ö†Ô∏è  High correlations may indicate multicollinearity issues.\")\n",
    "    print(\"Consider feature selection or dimensionality reduction techniques.\")\n",
    "else:\n",
    "    print(\"‚úÖ No high correlations found (|r| > 0.7). Good for avoiding multicollinearity.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze correlations with target variable\n",
    "if target_col in correlation_result.correlation_matrix.columns:\n",
    "    target_correlations = correlation_result.correlation_matrix[target_col].drop(target_col)\n",
    "    target_correlations = target_correlations.sort_values(key=abs, ascending=False)\n",
    "    \n",
    "    print(f\"=== Correlations with Target Variable ({target_col}) ===\")\n",
    "    print(\"\\nTop 10 strongest correlations:\")\n",
    "    \n",
    "    target_corr_df = pd.DataFrame({\n",
    "        'Feature': target_correlations.index,\n",
    "        'Correlation': target_correlations.values,\n",
    "        'Abs_Correlation': target_correlations.abs().values\n",
    "    }).head(10)\n",
    "    \n",
    "    display(target_corr_df.drop('Abs_Correlation', axis=1))\n",
    "    \n",
    "    # Visualize top correlations with target\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    top_10 = target_correlations.head(10)\n",
    "    colors = ['red' if x < 0 else 'blue' for x in top_10.values]\n",
    "    \n",
    "    bars = plt.barh(range(len(top_10)), top_10.values, color=colors, alpha=0.7)\n",
    "    plt.yticks(range(len(top_10)), [name.replace('_', ' ') for name in top_10.index])\n",
    "    plt.xlabel(f'Correlation with {target_col}')\n",
    "    plt.title(f'Top 10 Feature Correlations with {target_col}')\n",
    "    plt.axvline(x=0, color='black', linestyle='-', alpha=0.3)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, bar in enumerate(bars):\n",
    "        width = bar.get_width()\n",
    "        plt.text(width + (0.01 if width >= 0 else -0.01), bar.get_y() + bar.get_height()/2,\n",
    "                f'{width:.3f}', ha='left' if width >= 0 else 'right', va='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(f\"Target variable '{target_col}' not found in correlation matrix.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Statistical Testing {#testing}\n",
    "\n",
    "Let's perform comprehensive statistical testing with proper multiple testing corrections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize statistical tester\n",
    "tester = StatisticalTester(significance_level=0.05, multiple_testing_correction='bonferroni')\n",
    "\n",
    "# Perform comprehensive testing\n",
    "print(\"Performing comprehensive statistical testing...\")\n",
    "test_results = tester.perform_comprehensive_testing(telco_clean, target_col, feature_cols)\n",
    "\n",
    "print(f\"\\nCompleted testing for {len(test_results)} features\")\n",
    "\n",
    "# Calculate effect sizes\n",
    "print(\"Calculating effect sizes...\")\n",
    "effect_sizes = tester.calculate_effect_sizes(telco_clean, target_col, feature_cols)\n",
    "\n",
    "print(f\"Calculated effect sizes for {len(effect_sizes)} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive testing report\n",
    "testing_report = tester.generate_testing_report(test_results, effect_sizes)\n",
    "\n",
    "print(\"=== Statistical Testing Report ===\")\n",
    "display(testing_report)\n",
    "\n",
    "# Save the report\n",
    "report_path = config.TABLES_PATH / 'statistical_testing_report.csv'\n",
    "testing_report.to_csv(report_path, index=False)\n",
    "print(f\"\\nReport saved to: {report_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze multiple testing correction impact\n",
    "if 'p_value_corrected' in testing_report.columns:\n",
    "    print(\"=== Multiple Testing Correction Impact ===\")\n",
    "    \n",
    "    original_significant = testing_report['is_significant'].sum()\n",
    "    corrected_significant = testing_report['is_significant_corrected'].sum()\n",
    "    \n",
    "    print(f\"Original significant features: {original_significant}\")\n",
    "    print(f\"Significant after correction: {corrected_significant}\")\n",
    "    print(f\"Features lost due to correction: {original_significant - corrected_significant}\")\n",
    "    \n",
    "    if corrected_significant > 0:\n",
    "        print(\"\\nFeatures remaining significant after correction:\")\n",
    "        remaining_significant = testing_report[testing_report['is_significant_corrected'] == True]\n",
    "        for _, row in remaining_significant.iterrows():\n",
    "            print(f\"  - {row['feature']}: p={row['p_value']:.3e} ‚Üí p_corrected={row['p_value_corrected']:.3e}\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è  No features remain significant after multiple testing correction.\")\n",
    "        print(\"Consider using a less conservative correction method or increasing sample size.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Key Insights and Business Recommendations {#insights}\n",
    "\n",
    "Based on our comprehensive EDA, let's summarize the key findings and provide actionable business recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate analysis summary\n",
    "analysis_summary = eda_analyzer.get_analysis_summary()\n",
    "\n",
    "print(\"=== EDA Analysis Summary ===\")\n",
    "print(f\"Analysis timestamp: {analysis_summary['timestamp']}\")\n",
    "print(f\"Analyses performed: {', '.join(analysis_summary['analyses_performed'])}\")\n",
    "\n",
    "if 'univariate_summary' in analysis_summary:\n",
    "    uni_summary = analysis_summary['univariate_summary']\n",
    "    print(f\"\\nUnivariate Analysis:\")\n",
    "    print(f\"  - Total columns analyzed: {uni_summary['total_columns']}\")\n",
    "    print(f\"  - Numeric columns: {uni_summary['numeric_columns']}\")\n",
    "    print(f\"  - Categorical columns: {uni_summary['categorical_columns']}\")\n",
    "    print(f\"  - Columns with missing values: {uni_summary['columns_with_missing']}\")\n",
    "\n",
    "if 'bivariate_summary' in analysis_summary:\n",
    "    bi_summary = analysis_summary['bivariate_summary']\n",
    "    print(f\"\\nBivariate Analysis:\")\n",
    "    print(f\"  - Total features tested: {bi_summary['total_features']}\")\n",
    "    print(f\"  - Significant features: {bi_summary['significant_features']}\")\n",
    "    if bi_summary['significant_features'] > 0:\n",
    "        print(f\"  - Significant feature names: {', '.join(bi_summary['significant_feature_names'])}\")\n",
    "\n",
    "if 'correlation_summary' in analysis_summary:\n",
    "    corr_summary = analysis_summary['correlation_summary']\n",
    "    print(f\"\\nCorrelation Analysis:\")\n",
    "    print(f\"  - Method used: {corr_summary['correlation_method']}\")\n",
    "    print(f\"  - High correlations found: {corr_summary['high_correlations_count']}\")\n",
    "    print(f\"  - Significant correlations: {corr_summary['significant_correlations_count']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify key business insights\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"KEY BUSINESS INSIGHTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Churn rate analysis\n",
    "overall_churn_rate = telco_clean[target_col].mean()\n",
    "print(f\"\\n1. OVERALL CHURN RATE: {overall_churn_rate:.1%}\")\n",
    "\n",
    "if overall_churn_rate > 0.25:\n",
    "    print(\"   ‚ö†Ô∏è  HIGH CHURN RATE - Immediate attention required\")\n",
    "elif overall_churn_rate > 0.15:\n",
    "    print(\"   ‚ö° MODERATE CHURN RATE - Monitor closely\")\n",
    "else:\n",
    "    print(\"   ‚úÖ ACCEPTABLE CHURN RATE - Continue monitoring\")\n",
    "\n",
    "# 2. Most impactful features\n",
    "print(\"\\n2. MOST IMPACTFUL FEATURES:\")\n",
    "if len(significant_features) > 0:\n",
    "    top_features = significant_features.head(5)\n",
    "    for i, (_, row) in enumerate(top_features.iterrows()):\n",
    "        print(f\"   {i+1}. {row['Feature']} ({row['Type']})\")\n",
    "        print(f\"      - Statistical significance: p={row['P-value']}\")\n",
    "        print(f\"      - Effect size: {row['Effect Interpretation']}\")\n",
    "else:\n",
    "    print(\"   No statistically significant features identified.\")\n",
    "\n",
    "# 3. Data quality insights\n",
    "print(\"\\n3. DATA QUALITY ASSESSMENT:\")\n",
    "missing_vars = [name for name, result in univariate_results.items() if result.missing_count > 0]\n",
    "if missing_vars:\n",
    "    print(f\"   ‚ö†Ô∏è  Variables with missing data: {len(missing_vars)}\")\n",
    "    for var in missing_vars[:3]:  # Show top 3\n",
    "        result = univariate_results[var]\n",
    "        print(f\"      - {var}: {result.missing_percentage:.1f}% missing\")\n",
    "else:\n",
    "    print(\"   ‚úÖ No missing data detected\")\n",
    "\n",
    "# 4. Correlation insights\n",
    "print(\"\\n4. FEATURE RELATIONSHIPS:\")\n",
    "if correlation_result.high_correlations:\n",
    "    print(f\"   ‚ö†Ô∏è  {len(correlation_result.high_correlations)} high correlations detected\")\n",
    "    print(\"      - May indicate multicollinearity issues\")\n",
    "    print(\"      - Consider feature selection for modeling\")\n",
    "else:\n",
    "    print(\"   ‚úÖ No concerning correlations detected\")\n",
    "    print(\"      - Features appear to be relatively independent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate business recommendations\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BUSINESS RECOMMENDATIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "recommendations = []\n",
    "\n",
    "# Recommendation 1: Focus on significant features\n",
    "if len(significant_features) > 0:\n",
    "    top_feature = significant_features.iloc[0]['Feature']\n",
    "    recommendations.append(\n",
    "        f\"1. PRIORITIZE {top_feature.upper()} MANAGEMENT\\n\"\n",
    "        f\"   - This is the most statistically significant predictor of churn\\n\"\n",
    "        f\"   - Develop targeted interventions around this feature\\n\"\n",
    "        f\"   - Monitor changes in this variable closely\"\n",
    "    )\n",
    "\n",
    "# Recommendation 2: Address high-risk segments\n",
    "if len(categorical_significant) > 0:\n",
    "    recommendations.append(\n",
    "        \"2. SEGMENT-SPECIFIC RETENTION STRATEGIES\\n\"\n",
    "        \"   - Develop tailored retention programs for high-risk segments\\n\"\n",
    "        \"   - Focus resources on segments with highest churn rates\\n\"\n",
    "        \"   - Create early warning systems for at-risk customers\"\n",
    "    )\n",
    "\n",
    "# Recommendation 3: Data quality improvements\n",
    "if missing_vars:\n",
    "    recommendations.append(\n",
    "        \"3. IMPROVE DATA COLLECTION\\n\"\n",
    "        f\"   - Address missing data in {len(missing_vars)} variables\\n\"\n",
    "        \"   - Implement data validation at point of collection\\n\"\n",
    "        \"   - Consider imputation strategies for modeling\"\n",
    "    )\n",
    "\n",
    "# Recommendation 4: Feature engineering\n",
    "if correlation_result.high_correlations:\n",
    "    recommendations.append(\n",
    "        \"4. FEATURE ENGINEERING FOR MODELING\\n\"\n",
    "        \"   - Address multicollinearity through feature selection\\n\"\n",
    "        \"   - Consider creating composite features\\n\"\n",
    "        \"   - Use dimensionality reduction techniques if needed\"\n",
    "    )\n",
    "\n",
    "# Recommendation 5: Monitoring and tracking\n",
    "recommendations.append(\n",
    "    \"5. IMPLEMENT CONTINUOUS MONITORING\\n\"\n",
    "    \"   - Set up automated EDA reports for new data\\n\"\n",
    "    \"   - Track feature importance over time\\n\"\n",
    "    \"   - Monitor for data drift and concept drift\"\n",
    ")\n",
    "\n",
    "for rec in recommendations:\n",
    "    print(f\"\\n{rec}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusions {#conclusions}\n",
    "\n",
    "Let's create a comprehensive EDA dashboard and summarize our findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating comprehensive EDA dashboard...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'visualizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Create comprehensive EDA dashboard\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mCreating comprehensive EDA dashboard...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m dashboard_figures = \u001b[43mvisualizer\u001b[49m.create_eda_dashboard(\n\u001b[32m      5\u001b[39m     telco_clean, target_col, univariate_results, bivariate_results, \n\u001b[32m      6\u001b[39m     correlation_result, save_name=\u001b[33m'\u001b[39m\u001b[33meda_dashboard\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      7\u001b[39m )\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mCreated \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(dashboard_figures)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m dashboard figures\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Display all dashboard figures\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'visualizer' is not defined"
     ]
    }
   ],
   "source": [
    "# Create comprehensive EDA dashboard\n",
    "print(\"Creating comprehensive EDA dashboard...\")\n",
    "\n",
    "dashboard_figures = visualizer.create_eda_dashboard(\n",
    "    telco_clean, target_col, univariate_results, bivariate_results, \n",
    "    correlation_result, save_name='eda_dashboard'\n",
    ")\n",
    "\n",
    "print(f\"\\nCreated {len(dashboard_figures)} dashboard figures\")\n",
    "\n",
    "# Display all dashboard figures\n",
    "for i, fig in enumerate(dashboard_figures):\n",
    "    print(f\"\\n--- Dashboard Figure {i+1} ---\")\n",
    "    plt.show()\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary and next steps\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXPLORATORY DATA ANALYSIS - FINAL SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüìä DATASET OVERVIEW:\")\n",
    "print(f\"   - Samples: {len(telco_clean):,}\")\n",
    "print(f\"   - Features: {len(feature_cols)}\")\n",
    "print(f\"   - Churn rate: {overall_churn_rate:.1%}\")\n",
    "print(f\"   - Data quality: {'Good' if len(missing_vars) == 0 else 'Needs attention'}\")\n",
    "\n",
    "print(f\"\\nüîç KEY FINDINGS:\")\n",
    "print(f\"   - Significant predictors identified: {len(significant_features)}\")\n",
    "print(f\"   - High correlations detected: {len(correlation_result.high_correlations)}\")\n",
    "print(f\"   - Statistical tests performed: {len(test_results)}\")\n",
    "print(f\"   - Effect sizes calculated: {len(effect_sizes)}\")\n",
    "\n",
    "print(f\"\\nüìà BUSINESS IMPACT:\")\n",
    "if len(significant_features) > 0:\n",
    "    print(f\"   - Clear drivers of churn identified\")\n",
    "    print(f\"   - Actionable insights for retention strategies\")\n",
    "    print(f\"   - Strong foundation for predictive modeling\")\n",
    "else:\n",
    "    print(f\"   - No clear statistical drivers identified\")\n",
    "    print(f\"   - May need more data or different features\")\n",
    "    print(f\"   - Consider advanced feature engineering\")\n",
    "\n",
    "print(f\"\\nüöÄ NEXT STEPS:\")\n",
    "print(f\"   1. Proceed with feature engineering based on EDA insights\")\n",
    "print(f\"   2. Address data quality issues identified\")\n",
    "print(f\"   3. Develop predictive models using significant features\")\n",
    "print(f\"   4. Create business rules based on segment analysis\")\n",
    "print(f\"   5. Set up monitoring for key churn indicators\")\n",
    "\n",
    "print(f\"\\nüìÅ OUTPUTS GENERATED:\")\n",
    "print(f\"   - Statistical testing report: {config.TABLES_PATH / 'statistical_testing_report.csv'}\")\n",
    "print(f\"   - EDA visualizations: {config.FIGURES_PATH}\")\n",
    "print(f\"   - Analysis results stored in memory for next steps\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EDA ANALYSIS COMPLETE ‚úÖ\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "This comprehensive EDA notebook has provided:\n",
    "\n",
    "1. **Complete data profiling** with quality assessment\n",
    "2. **Univariate analysis** of all features with appropriate statistics\n",
    "3. **Bivariate analysis** with statistical testing against the target\n",
    "4. **Correlation analysis** using appropriate measures for mixed data types\n",
    "5. **Statistical significance testing** with multiple testing corrections\n",
    "6. **Publication-ready visualizations** automatically saved to reports\n",
    "7. **Business insights and actionable recommendations**\n",
    "\n",
    "The analysis framework is designed to be:\n",
    "- **Reproducible**: All random seeds set and parameters configurable\n",
    "- **Scalable**: Works with different datasets and sizes\n",
    "- **Comprehensive**: Covers all major aspects of EDA\n",
    "- **Business-focused**: Provides actionable insights\n",
    "\n",
    "**Next Steps**: Use these insights to guide feature engineering and model development in subsequent notebooks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
